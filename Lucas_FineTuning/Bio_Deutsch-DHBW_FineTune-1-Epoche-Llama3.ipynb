{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "Copy-and-paste the text below in your GitHub issue and FILL OUT the two last points.\n",
      "\n",
      "- `transformers` version: 4.38.1\n",
      "- Platform: Linux-5.4.0-153-generic-x86_64-with-glibc2.35\n",
      "- Python version: 3.10.12\n",
      "- Huggingface_hub version: 0.20.3\n",
      "- Safetensors version: 0.4.3\n",
      "- Accelerate version: 0.27.2\n",
      "- Accelerate config: \tnot found\n",
      "- PyTorch version (GPU?): 2.1.1+cu121 (True)\n",
      "- Tensorflow version (GPU?): not installed (NA)\n",
      "- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n",
      "- Jax version: not installed\n",
      "- JaxLib version: not installed\n",
      "- Using GPU in script?: <fill in>\n",
      "- Using distributed or parallel set-up in script?: <fill in>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip -q\n",
    "!pip install transformers==4.38.1 -q -U\n",
    "!pip install bitsandbytes==0.42.0 -q -U\n",
    "!pip install peft==0.8.2 -q -U\n",
    "!pip install accelerate==0.27.2 -q -U\n",
    "!pip install flash-attn==2.5.5 -q -U\n",
    "!pip install datasets==2.17.1 -q -U\n",
    "!pip install scipy==1.12.0 -q -U\n",
    "!pip install trl==0.7.11 -q -U\n",
    "!pip install hf_transfer==0.1.5 -q -U\n",
    "!pip install huggingface_hub==0.20.3 -q -U\n",
    "!pip install wandb==0.16.3 -q -U\n",
    "\n",
    "\n",
    "!transformers-cli env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269364d0d0d44461aa77af627deb557c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Required when training models/data that are gated on HuggingFace, and required for pushing models to HuggingFace\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_HUB_ENABLE_HF_TRANSFER=True\n"
     ]
    }
   ],
   "source": [
    "%env HF_HUB_ENABLE_HF_TRANSFER=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "c8dd41732ce7463694555f3486afe470",
      "6433bf5eed7d4bd28a954df13b6f0e94",
      "18180c98496e48f2861cade3dbaf8364",
      "68892d09649a45d193192824dd8f37f5",
      "6834175de25d453f92164ebb336de22b",
      "589a72488d9c4553a18761d6966c11a2",
      "0f211d88f23a4fac9a20aaa2262240f1",
      "aaf181f19b93404f9553cb222e531ce9",
      "86587e97d3c043c99495c35d7505884b",
      "848d078ed62443e297125e6532032549",
      "6c056bb1731546598a7cdddbab3aaa07",
      "c7f822a2863d45698f4a172fa3d720b6",
      "b90890729a954284ac2e6d1a7289d488",
      "559f7b17cf8d479b89134fbac2ff0de9",
      "7ea1b3fcf3174b41b1d38155929526fb",
      "bcfb9c09464546b6aeca67d53badedcf",
      "3fbaceee3ccf4a64b80d9451d5aa5701",
      "b5afaad1a4e74b0a923cb70fa57377dc",
      "d8eabbb772c347a69ddc2cdeb2b4c01f",
      "2ef6ac7be9464d128f89b69b28e25b40",
      "06c6dd0b25764569aacdfa2bb01f696f",
      "aeb9f39902a341c59b8d346a2558d1d1",
      "387a6dba93c54678a3a5404210afd04c",
      "e5923556c18240578703e5d443e6c81c",
      "e909f9afd82d423a967dda899fb2bf1b",
      "465bf049acb64c52933fbfe50f7d4ec7",
      "ac6f2cd6604c4c9996ca09e233dd6dbe",
      "8fca51abfe404f39a540b39df8df55b0",
      "a8599580508045cfa97149579641ce83",
      "ce690f5848a347f0860e9c3d1704f021",
      "09597bc3af6144eb8ecf874aa4dce884",
      "14c7809be6de472dad580fb54ad9ca7b",
      "043f148d952e482a9029986f99161944"
     ]
    },
    "id": "E0Nl5mWL0k2T",
    "outputId": "be6ef436-747e-49eb-d211-e369e8f18408"
   },
   "outputs": [],
   "source": [
    "cache_dir='' \n",
    "# model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model_id = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "# model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model_id = \"NousResearch/Meta-Llama-3-8B-Instruct\"\n",
    "# model_id = \"Qwen/Qwen1.5-7B-Chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5134ca55ebf8420ba3b825f9e9707be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8783621b825843f6b2ebd97d73eae441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d10b0ed4d54cbfa5f8e240f18687ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a98858ba24b4abf97b64c345b58e1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785ae9d36d3347b48944f5405c6d77b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163feefbe6504fceb356c9247b1c5aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a9dd34224d411d95abffb471e578e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67883fd306944e4f800ff78a2ead6bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389c62b272394b5abba4efcd0f9793c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6978b1d3213e438ebafdab96123f3928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/126 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74829405c2164cc3bc4a6c770f00884b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37acaebc1b764a2a8addc6f11c0368ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc8b2a36fdf4174bbd0ffbf4e32f679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "config.max_position_embeddings = 4096 \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\", \n",
    "    cache_dir=cache_dir)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,use_fast=True,trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in model.named_parameters():\n",
    "    if p.device.type == \"meta\":\n",
    "        print(f\"{n} is on meta!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192\n",
      "128001\n"
     ]
    }
   ],
   "source": [
    "print(model.config.max_position_embeddings)\n",
    "print(model.config.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Printed die Zahl der trainierbaren Parameter\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    non_trainable_params = 0\n",
    "    all_params = 0\n",
    "\n",
    "    print(\"Trainable Parameters:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "            print(f\"  {name}\")\n",
    "        else:\n",
    "            non_trainable_params += param.numel()\n",
    "\n",
    "    print(\"\\nNon-Trainable Parameters:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            print(f\"  {name}\")\n",
    "\n",
    "    print(\n",
    "        f\"\\nSummary:\\n  Trainable params: {trainable_params}\\n  Non-Trainable params: {non_trainable_params}\\n  All params: {all_params}\\n  Trainable%: {100 * trainable_params / all_params}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable() #Zum speichern von VRAM\n",
    "# model = prepare_model_for_kbit_training(model) # Fuer Quantisierung\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig( #matching the Llama recipe\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        # \"self_attn.rotary_emb.inv_freq\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # use_dora=True # only for DoRA\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config) #move to a peft model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters:\n",
      "  base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight\n",
      "  base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight\n",
      "  base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight\n",
      "\n",
      "Non-Trainable Parameters:\n",
      "  base_model.model.model.embed_tokens.weight\n",
      "  base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.0.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.0.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.0.input_layernorm.weight\n",
      "  base_model.model.model.layers.0.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.1.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.1.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.1.input_layernorm.weight\n",
      "  base_model.model.model.layers.1.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.2.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.2.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.2.input_layernorm.weight\n",
      "  base_model.model.model.layers.2.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.3.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.3.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.3.input_layernorm.weight\n",
      "  base_model.model.model.layers.3.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.4.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.4.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.4.input_layernorm.weight\n",
      "  base_model.model.model.layers.4.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.5.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.5.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.5.input_layernorm.weight\n",
      "  base_model.model.model.layers.5.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.6.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.6.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.6.input_layernorm.weight\n",
      "  base_model.model.model.layers.6.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.7.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.7.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.7.input_layernorm.weight\n",
      "  base_model.model.model.layers.7.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.8.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.8.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.8.input_layernorm.weight\n",
      "  base_model.model.model.layers.8.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.9.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.9.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.9.input_layernorm.weight\n",
      "  base_model.model.model.layers.9.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.10.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.10.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.10.input_layernorm.weight\n",
      "  base_model.model.model.layers.10.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.11.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.11.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.11.input_layernorm.weight\n",
      "  base_model.model.model.layers.11.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.12.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.12.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.12.input_layernorm.weight\n",
      "  base_model.model.model.layers.12.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.13.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.13.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.13.input_layernorm.weight\n",
      "  base_model.model.model.layers.13.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.14.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.14.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.14.input_layernorm.weight\n",
      "  base_model.model.model.layers.14.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.15.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.15.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.15.input_layernorm.weight\n",
      "  base_model.model.model.layers.15.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.16.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.16.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.16.input_layernorm.weight\n",
      "  base_model.model.model.layers.16.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.17.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.17.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.17.input_layernorm.weight\n",
      "  base_model.model.model.layers.17.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.18.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.18.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.18.input_layernorm.weight\n",
      "  base_model.model.model.layers.18.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.19.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.19.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.19.input_layernorm.weight\n",
      "  base_model.model.model.layers.19.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.20.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.20.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.20.input_layernorm.weight\n",
      "  base_model.model.model.layers.20.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.21.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.21.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.21.input_layernorm.weight\n",
      "  base_model.model.model.layers.21.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.22.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.22.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.22.input_layernorm.weight\n",
      "  base_model.model.model.layers.22.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.23.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.23.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.23.input_layernorm.weight\n",
      "  base_model.model.model.layers.23.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.24.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.24.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.24.input_layernorm.weight\n",
      "  base_model.model.model.layers.24.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.25.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.25.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.25.input_layernorm.weight\n",
      "  base_model.model.model.layers.25.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.26.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.26.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.26.input_layernorm.weight\n",
      "  base_model.model.model.layers.26.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.27.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.27.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.27.input_layernorm.weight\n",
      "  base_model.model.model.layers.27.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.28.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.28.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.28.input_layernorm.weight\n",
      "  base_model.model.model.layers.28.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.29.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.29.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.29.input_layernorm.weight\n",
      "  base_model.model.model.layers.29.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.30.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.30.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.30.input_layernorm.weight\n",
      "  base_model.model.model.layers.30.post_attention_layernorm.weight\n",
      "  base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight\n",
      "  base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight\n",
      "  base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight\n",
      "  base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight\n",
      "  base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight\n",
      "  base_model.model.model.layers.31.mlp.up_proj.base_layer.weight\n",
      "  base_model.model.model.layers.31.mlp.down_proj.base_layer.weight\n",
      "  base_model.model.model.layers.31.input_layernorm.weight\n",
      "  base_model.model.model.layers.31.post_attention_layernorm.weight\n",
      "  base_model.model.model.norm.weight\n",
      "  base_model.model.lm_head.weight\n",
      "\n",
      "Summary:\n",
      "  Trainable params: 20971520\n",
      "  Non-Trainable params: 7241748480\n",
      "  All params: 7262720000\n",
      "  Trainable%: 0.28875572788156506\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='NousResearch/Meta-Llama-3-8B-Instruct', vocab_size=128000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|begin_of_text|>', 'eos_token': '<|end_of_text|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t128000: AddedToken(\"<|begin_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128001: AddedToken(\"<|end_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128002: AddedToken(\"<|reserved_special_token_0|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128003: AddedToken(\"<|reserved_special_token_1|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128004: AddedToken(\"<|reserved_special_token_2|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128005: AddedToken(\"<|reserved_special_token_3|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128006: AddedToken(\"<|start_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128007: AddedToken(\"<|end_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128008: AddedToken(\"<|reserved_special_token_4|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128009: AddedToken(\"<|eot_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128010: AddedToken(\"<|reserved_special_token_5|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128011: AddedToken(\"<|reserved_special_token_6|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128012: AddedToken(\"<|reserved_special_token_7|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128013: AddedToken(\"<|reserved_special_token_8|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128014: AddedToken(\"<|reserved_special_token_9|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128015: AddedToken(\"<|reserved_special_token_10|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128016: AddedToken(\"<|reserved_special_token_11|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128017: AddedToken(\"<|reserved_special_token_12|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128018: AddedToken(\"<|reserved_special_token_13|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128019: AddedToken(\"<|reserved_special_token_14|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128020: AddedToken(\"<|reserved_special_token_15|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128021: AddedToken(\"<|reserved_special_token_16|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128022: AddedToken(\"<|reserved_special_token_17|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128023: AddedToken(\"<|reserved_special_token_18|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128024: AddedToken(\"<|reserved_special_token_19|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128025: AddedToken(\"<|reserved_special_token_20|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128026: AddedToken(\"<|reserved_special_token_21|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128027: AddedToken(\"<|reserved_special_token_22|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128028: AddedToken(\"<|reserved_special_token_23|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128029: AddedToken(\"<|reserved_special_token_24|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128030: AddedToken(\"<|reserved_special_token_25|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128031: AddedToken(\"<|reserved_special_token_26|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128032: AddedToken(\"<|reserved_special_token_27|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128033: AddedToken(\"<|reserved_special_token_28|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128034: AddedToken(\"<|reserved_special_token_29|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128035: AddedToken(\"<|reserved_special_token_30|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128036: AddedToken(\"<|reserved_special_token_31|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128037: AddedToken(\"<|reserved_special_token_32|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128038: AddedToken(\"<|reserved_special_token_33|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128039: AddedToken(\"<|reserved_special_token_34|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128040: AddedToken(\"<|reserved_special_token_35|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128041: AddedToken(\"<|reserved_special_token_36|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128042: AddedToken(\"<|reserved_special_token_37|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128043: AddedToken(\"<|reserved_special_token_38|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128044: AddedToken(\"<|reserved_special_token_39|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128045: AddedToken(\"<|reserved_special_token_40|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128046: AddedToken(\"<|reserved_special_token_41|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128047: AddedToken(\"<|reserved_special_token_42|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128048: AddedToken(\"<|reserved_special_token_43|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128049: AddedToken(\"<|reserved_special_token_44|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128050: AddedToken(\"<|reserved_special_token_45|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128051: AddedToken(\"<|reserved_special_token_46|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128052: AddedToken(\"<|reserved_special_token_47|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128053: AddedToken(\"<|reserved_special_token_48|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128054: AddedToken(\"<|reserved_special_token_49|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128055: AddedToken(\"<|reserved_special_token_50|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128056: AddedToken(\"<|reserved_special_token_51|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128057: AddedToken(\"<|reserved_special_token_52|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128058: AddedToken(\"<|reserved_special_token_53|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128059: AddedToken(\"<|reserved_special_token_54|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128060: AddedToken(\"<|reserved_special_token_55|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128061: AddedToken(\"<|reserved_special_token_56|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128062: AddedToken(\"<|reserved_special_token_57|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128063: AddedToken(\"<|reserved_special_token_58|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128064: AddedToken(\"<|reserved_special_token_59|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128065: AddedToken(\"<|reserved_special_token_60|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128066: AddedToken(\"<|reserved_special_token_61|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128067: AddedToken(\"<|reserved_special_token_62|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128068: AddedToken(\"<|reserved_special_token_63|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128069: AddedToken(\"<|reserved_special_token_64|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128070: AddedToken(\"<|reserved_special_token_65|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128071: AddedToken(\"<|reserved_special_token_66|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128072: AddedToken(\"<|reserved_special_token_67|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128073: AddedToken(\"<|reserved_special_token_68|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128074: AddedToken(\"<|reserved_special_token_69|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128075: AddedToken(\"<|reserved_special_token_70|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128076: AddedToken(\"<|reserved_special_token_71|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128077: AddedToken(\"<|reserved_special_token_72|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128078: AddedToken(\"<|reserved_special_token_73|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128079: AddedToken(\"<|reserved_special_token_74|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128080: AddedToken(\"<|reserved_special_token_75|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128081: AddedToken(\"<|reserved_special_token_76|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128082: AddedToken(\"<|reserved_special_token_77|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128083: AddedToken(\"<|reserved_special_token_78|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128084: AddedToken(\"<|reserved_special_token_79|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128085: AddedToken(\"<|reserved_special_token_80|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128086: AddedToken(\"<|reserved_special_token_81|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128087: AddedToken(\"<|reserved_special_token_82|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128088: AddedToken(\"<|reserved_special_token_83|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128089: AddedToken(\"<|reserved_special_token_84|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128090: AddedToken(\"<|reserved_special_token_85|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128091: AddedToken(\"<|reserved_special_token_86|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128092: AddedToken(\"<|reserved_special_token_87|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128093: AddedToken(\"<|reserved_special_token_88|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128094: AddedToken(\"<|reserved_special_token_89|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128095: AddedToken(\"<|reserved_special_token_90|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128096: AddedToken(\"<|reserved_special_token_91|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128097: AddedToken(\"<|reserved_special_token_92|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128098: AddedToken(\"<|reserved_special_token_93|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128099: AddedToken(\"<|reserved_special_token_94|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128100: AddedToken(\"<|reserved_special_token_95|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128101: AddedToken(\"<|reserved_special_token_96|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128102: AddedToken(\"<|reserved_special_token_97|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128103: AddedToken(\"<|reserved_special_token_98|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128104: AddedToken(\"<|reserved_special_token_99|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128105: AddedToken(\"<|reserved_special_token_100|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128106: AddedToken(\"<|reserved_special_token_101|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128107: AddedToken(\"<|reserved_special_token_102|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128108: AddedToken(\"<|reserved_special_token_103|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128109: AddedToken(\"<|reserved_special_token_104|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128110: AddedToken(\"<|reserved_special_token_105|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128111: AddedToken(\"<|reserved_special_token_106|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128112: AddedToken(\"<|reserved_special_token_107|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128113: AddedToken(\"<|reserved_special_token_108|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128114: AddedToken(\"<|reserved_special_token_109|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128115: AddedToken(\"<|reserved_special_token_110|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128116: AddedToken(\"<|reserved_special_token_111|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128117: AddedToken(\"<|reserved_special_token_112|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128118: AddedToken(\"<|reserved_special_token_113|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128119: AddedToken(\"<|reserved_special_token_114|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128120: AddedToken(\"<|reserved_special_token_115|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128121: AddedToken(\"<|reserved_special_token_116|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128122: AddedToken(\"<|reserved_special_token_117|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128123: AddedToken(\"<|reserved_special_token_118|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128124: AddedToken(\"<|reserved_special_token_119|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128125: AddedToken(\"<|reserved_special_token_120|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128126: AddedToken(\"<|reserved_special_token_121|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128127: AddedToken(\"<|reserved_special_token_122|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128128: AddedToken(\"<|reserved_special_token_123|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128129: AddedToken(\"<|reserved_special_token_124|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128130: AddedToken(\"<|reserved_special_token_125|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128131: AddedToken(\"<|reserved_special_token_126|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128132: AddedToken(\"<|reserved_special_token_127|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128133: AddedToken(\"<|reserved_special_token_128|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128134: AddedToken(\"<|reserved_special_token_129|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128135: AddedToken(\"<|reserved_special_token_130|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128136: AddedToken(\"<|reserved_special_token_131|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128137: AddedToken(\"<|reserved_special_token_132|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128138: AddedToken(\"<|reserved_special_token_133|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128139: AddedToken(\"<|reserved_special_token_134|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128140: AddedToken(\"<|reserved_special_token_135|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128141: AddedToken(\"<|reserved_special_token_136|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128142: AddedToken(\"<|reserved_special_token_137|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128143: AddedToken(\"<|reserved_special_token_138|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128144: AddedToken(\"<|reserved_special_token_139|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128145: AddedToken(\"<|reserved_special_token_140|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128146: AddedToken(\"<|reserved_special_token_141|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128147: AddedToken(\"<|reserved_special_token_142|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128148: AddedToken(\"<|reserved_special_token_143|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128149: AddedToken(\"<|reserved_special_token_144|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128150: AddedToken(\"<|reserved_special_token_145|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128151: AddedToken(\"<|reserved_special_token_146|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128152: AddedToken(\"<|reserved_special_token_147|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128153: AddedToken(\"<|reserved_special_token_148|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128154: AddedToken(\"<|reserved_special_token_149|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128155: AddedToken(\"<|reserved_special_token_150|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128156: AddedToken(\"<|reserved_special_token_151|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128157: AddedToken(\"<|reserved_special_token_152|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128158: AddedToken(\"<|reserved_special_token_153|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128159: AddedToken(\"<|reserved_special_token_154|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128160: AddedToken(\"<|reserved_special_token_155|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128161: AddedToken(\"<|reserved_special_token_156|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128162: AddedToken(\"<|reserved_special_token_157|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128163: AddedToken(\"<|reserved_special_token_158|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128164: AddedToken(\"<|reserved_special_token_159|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128165: AddedToken(\"<|reserved_special_token_160|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128166: AddedToken(\"<|reserved_special_token_161|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128167: AddedToken(\"<|reserved_special_token_162|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128168: AddedToken(\"<|reserved_special_token_163|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128169: AddedToken(\"<|reserved_special_token_164|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128170: AddedToken(\"<|reserved_special_token_165|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128171: AddedToken(\"<|reserved_special_token_166|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128172: AddedToken(\"<|reserved_special_token_167|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128173: AddedToken(\"<|reserved_special_token_168|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128174: AddedToken(\"<|reserved_special_token_169|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128175: AddedToken(\"<|reserved_special_token_170|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128176: AddedToken(\"<|reserved_special_token_171|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128177: AddedToken(\"<|reserved_special_token_172|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128178: AddedToken(\"<|reserved_special_token_173|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128179: AddedToken(\"<|reserved_special_token_174|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128180: AddedToken(\"<|reserved_special_token_175|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128181: AddedToken(\"<|reserved_special_token_176|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128182: AddedToken(\"<|reserved_special_token_177|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128183: AddedToken(\"<|reserved_special_token_178|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128184: AddedToken(\"<|reserved_special_token_179|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128185: AddedToken(\"<|reserved_special_token_180|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128186: AddedToken(\"<|reserved_special_token_181|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128187: AddedToken(\"<|reserved_special_token_182|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128188: AddedToken(\"<|reserved_special_token_183|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128189: AddedToken(\"<|reserved_special_token_184|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128190: AddedToken(\"<|reserved_special_token_185|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128191: AddedToken(\"<|reserved_special_token_186|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128192: AddedToken(\"<|reserved_special_token_187|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128193: AddedToken(\"<|reserved_special_token_188|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128194: AddedToken(\"<|reserved_special_token_189|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128195: AddedToken(\"<|reserved_special_token_190|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128196: AddedToken(\"<|reserved_special_token_191|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128197: AddedToken(\"<|reserved_special_token_192|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128198: AddedToken(\"<|reserved_special_token_193|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128199: AddedToken(\"<|reserved_special_token_194|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128200: AddedToken(\"<|reserved_special_token_195|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128201: AddedToken(\"<|reserved_special_token_196|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128202: AddedToken(\"<|reserved_special_token_197|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128203: AddedToken(\"<|reserved_special_token_198|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128204: AddedToken(\"<|reserved_special_token_199|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128205: AddedToken(\"<|reserved_special_token_200|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128206: AddedToken(\"<|reserved_special_token_201|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128207: AddedToken(\"<|reserved_special_token_202|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128208: AddedToken(\"<|reserved_special_token_203|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128209: AddedToken(\"<|reserved_special_token_204|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128210: AddedToken(\"<|reserved_special_token_205|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128211: AddedToken(\"<|reserved_special_token_206|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128212: AddedToken(\"<|reserved_special_token_207|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128213: AddedToken(\"<|reserved_special_token_208|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128214: AddedToken(\"<|reserved_special_token_209|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128215: AddedToken(\"<|reserved_special_token_210|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128216: AddedToken(\"<|reserved_special_token_211|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128217: AddedToken(\"<|reserved_special_token_212|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128218: AddedToken(\"<|reserved_special_token_213|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128219: AddedToken(\"<|reserved_special_token_214|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128220: AddedToken(\"<|reserved_special_token_215|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128221: AddedToken(\"<|reserved_special_token_216|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128222: AddedToken(\"<|reserved_special_token_217|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128223: AddedToken(\"<|reserved_special_token_218|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128224: AddedToken(\"<|reserved_special_token_219|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128225: AddedToken(\"<|reserved_special_token_220|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128226: AddedToken(\"<|reserved_special_token_221|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128227: AddedToken(\"<|reserved_special_token_222|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128228: AddedToken(\"<|reserved_special_token_223|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128229: AddedToken(\"<|reserved_special_token_224|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128230: AddedToken(\"<|reserved_special_token_225|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128231: AddedToken(\"<|reserved_special_token_226|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128232: AddedToken(\"<|reserved_special_token_227|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128233: AddedToken(\"<|reserved_special_token_228|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128234: AddedToken(\"<|reserved_special_token_229|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128235: AddedToken(\"<|reserved_special_token_230|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128236: AddedToken(\"<|reserved_special_token_231|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128237: AddedToken(\"<|reserved_special_token_232|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128238: AddedToken(\"<|reserved_special_token_233|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128239: AddedToken(\"<|reserved_special_token_234|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128240: AddedToken(\"<|reserved_special_token_235|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128241: AddedToken(\"<|reserved_special_token_236|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128242: AddedToken(\"<|reserved_special_token_237|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128243: AddedToken(\"<|reserved_special_token_238|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128244: AddedToken(\"<|reserved_special_token_239|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128245: AddedToken(\"<|reserved_special_token_240|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128246: AddedToken(\"<|reserved_special_token_241|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128247: AddedToken(\"<|reserved_special_token_242|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128248: AddedToken(\"<|reserved_special_token_243|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128249: AddedToken(\"<|reserved_special_token_244|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128250: AddedToken(\"<|reserved_special_token_245|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128251: AddedToken(\"<|reserved_special_token_246|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128252: AddedToken(\"<|reserved_special_token_247|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128253: AddedToken(\"<|reserved_special_token_248|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128254: AddedToken(\"<|reserved_special_token_249|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128255: AddedToken(\"<|reserved_special_token_250|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "128000\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>\n",
      "<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.bos_token)\n",
    "print(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "write a quick sort algorithm in python.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "here you are.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "great.<|eot_id|><|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "#Chat Template testen\n",
    "messages=[\n",
    "    { 'role': 'user', 'content': \"write a quick sort algorithm in python.\"},\n",
    "    { 'role': 'assistant', 'content': \"here you are.\"},\n",
    "    { 'role': 'user', 'content': \"great.\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using EOS token, <|end_of_text|>, for padding. WARNING, this may not be ideal for chat fine-tuning models.\n"
     ]
    }
   ],
   "source": [
    "## OPTION A - set the pad token to <pad>, if not <|pad|>, if not <unk> if <unk> is in the tokenizer OR set it to the EOS token.\n",
    "if '<pad>' in tokenizer.get_vocab():\n",
    "    print('<pad> token is in the tokenizer. Using <pad> for pad')\n",
    "    # Set the pad token\n",
    "    tokenizer.pad_token = '<pad>'\n",
    "elif '<|pad|>' in tokenizer.get_vocab():\n",
    "    print('<|pad|> token is in the tokenizer. Using <|pad|> for pad')\n",
    "    # Set the pad token\n",
    "    tokenizer.pad_token = '<|pad|>'\n",
    "elif '<unk>' in tokenizer.get_vocab():\n",
    "    print('<unk> token is in the tokenizer. Using unk for pad')\n",
    "    # Set the pad token\n",
    "    tokenizer.pad_token = '<unk>'\n",
    "else:\n",
    "    print(f'Using EOS token, {tokenizer.eos_token}, for padding. WARNING, this may not be ideal for chat fine-tuning models.')\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer pad token ID: 128001\n",
      "Model pad token ID: 128001\n",
      "Model config pad token ID: 128001\n",
      "Number of tokens now in tokenizer: 128000\n"
     ]
    }
   ],
   "source": [
    "# Update pad token id in model und die config\n",
    "model.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Checken ob sie gleich sind\n",
    "assert model.pad_token_id == tokenizer.pad_token_id, \"The model's pad token ID does not match the tokenizer's pad token ID!\"\n",
    "\n",
    "# Printe die pad token ids\n",
    "print('Tokenizer pad token ID:', tokenizer.pad_token_id)\n",
    "print('Model pad token ID:', model.pad_token_id)\n",
    "print('Model config pad token ID:', model.config.pad_token_id)\n",
    "print('Number of tokens now in tokenizer:', tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens map: {'bos_token': '<|begin_of_text|>', 'eos_token': '<|end_of_text|>', 'pad_token': '<|end_of_text|>'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Special tokens map:\", tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "sQ4dBAJOovzz"
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import gc  # \n",
    "\n",
    "# Definiere einen stream\n",
    "def stream(user_prompt, model_type, tokenizer, checkpoint=''):\n",
    "\n",
    "    if model_type == 'base':\n",
    "        eval_model = model\n",
    "    elif model_type == 'fine-tuned':\n",
    "        eval_model = PeftModel.from_pretrained(model, checkpoint) \n",
    "        eval_model = eval_model.to(\"cuda\")\n",
    "\n",
    "        for n, p in eval_model.named_parameters():\n",
    "            if p.device.type == \"cpu\":\n",
    "                print(f\"{n} is on cpu!\")\n",
    "        \n",
    "    else:\n",
    "        print('You must set the model_type to base or fine-tuned')\n",
    "        exit()  \n",
    "\n",
    "   \n",
    "    eval_model.config.use_cache = True\n",
    "\n",
    "    messages=[\n",
    "        { 'role': 'user', 'content': f\"{user_prompt.strip()}\"},\n",
    "    ]\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer([inputs], return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
    "\n",
    "    if \"token_type_ids\" in inputs:\n",
    "        del inputs[\"token_type_ids\"]\n",
    "    \n",
    "    streamer = TextStreamer(tokenizer)\n",
    "\n",
    "    print(f'eval_model is on: {next(eval_model.parameters()).device}')  # Debug line\n",
    "    print(f'input_ids are on: {inputs[\"input_ids\"].device}')  # Debug line\n",
    "\n",
    "\n",
    "    _ = eval_model.generate(**inputs, streamer=streamer, max_new_tokens=100, do_sample=False, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "   \n",
    "    torch.cuda.empty_cache()  \n",
    "    gc.collect()  # Lass garbage collection laufen\n",
    "\n",
    "\n",
    "def evaluation(model_type, tokenizer, checkpoint=''):\n",
    "    questions = [\n",
    "        \"What does Bitcoin mining achieve?\",\n",
    "        \"How are Bitcoin transactions verified?\",\n",
    "        \"What is the function of a Bitcoin wallet?\",\n",
    "        \"What triggers Bitcoin's price change?\",\n",
    "        \"What is a Bitcoin hard fork?\"\n",
    "    ]\n",
    "\n",
    "    answers = [\n",
    "         \"Validates transactions and secures the network by adding new blocks to the blockchain through solving SHA-256 hash problems.\",\n",
    "        \"Through digital signatures created with a sender's private key and confirmed by network nodes using the corresponding public key.\",\n",
    "        \"Manages private keys for signing transactions and tracks associated public addresses for receiving Bitcoin.\",\n",
    "        \"Supply and demand dynamics, influenced by factors like mining halvings, network activity, and macroeconomic indicators.\",\n",
    "        \"A protocol upgrade that splits the blockchain into two paths: one following new rules (incompatible with old software) and the original continuing as before.\"\n",
    "    ]\n",
    "\n",
    "    for question, answer in zip(questions, answers):\n",
    "        stream(question, model_type, tokenizer, checkpoint)\n",
    "        print(\"Correct Answer:\", answer)\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H8VWH8ZDPZ4U",
    "outputId": "1973e9f1-ffd9-4b3d-e27d-5a30853d9289"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_model is on: cuda:0\n",
      "input_ids are on: cuda:0\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What does Bitcoin mining achieve?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Bitcoin mining is the process of verifying and adding new transactions to the public ledger called the blockchain, while also solving complex mathematical problems to validate the transactions and secure the network. The main goals of Bitcoin mining are:\n",
      "\n",
      "1. **Verification of transactions**: Miners collect and verify a group of unconfirmed transactions, known as a block, to ensure that they are valid and legitimate. This involves checking that the sender has the necessary funds, that the transaction is properly formatted, and that the transaction is not attempting\n",
      "Correct Answer: Validates transactions and secures the network by adding new blocks to the blockchain through solving SHA-256 hash problems.\n",
      "\n",
      "\n",
      "\n",
      "eval_model is on: cuda:0\n",
      "input_ids are on: cuda:0\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How are Bitcoin transactions verified?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Bitcoin transactions are verified through a decentralized process called \"mining\" that involves a network of computers around the world. Here's a step-by-step explanation of how it works:\n",
      "\n",
      "1. **Transaction creation**: When a user wants to send Bitcoin to another user, they create a transaction and broadcast it to the Bitcoin network.\n",
      "2. **Transaction validation**: The transaction is verified by special nodes on the network called \"full nodes\" to ensure that:\n",
      "\t* The sender has the necessary Bitcoin balance to make\n",
      "Correct Answer: Through digital signatures created with a sender's private key and confirmed by network nodes using the corresponding public key.\n",
      "\n",
      "\n",
      "\n",
      "eval_model is on: cuda:0\n",
      "input_ids are on: cuda:0\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the function of a Bitcoin wallet?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "A Bitcoin wallet is a software program or physical device that allows users to store, send, and receive Bitcoins. The primary functions of a Bitcoin wallet are:\n",
      "\n",
      "1. **Storage**: A Bitcoin wallet stores the private keys and public addresses associated with a user's Bitcoin balance. These private keys are used to authorize transactions and ensure the security of the user's funds.\n",
      "2. **Transaction management**: A Bitcoin wallet allows users to send and receive Bitcoins by creating and signing transactions. This includes generating a unique address\n",
      "Correct Answer: Manages private keys for signing transactions and tracks associated public addresses for receiving Bitcoin.\n",
      "\n",
      "\n",
      "\n",
      "eval_model is on: cuda:0\n",
      "input_ids are on: cuda:0\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What triggers Bitcoin's price change?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Bitcoin's price is influenced by a complex array of factors, making it challenging to pinpoint a single trigger. However, here are some of the most significant factors that can impact Bitcoin's price:\n",
      "\n",
      "1. **Supply and Demand**: The balance between the number of people wanting to buy Bitcoin (demand) and the number of people willing to sell it (supply) can cause price fluctuations.\n",
      "2. **Global Economic Conditions**: Economic indicators such as inflation rates, interest rates, and GDP growth can affect investor sentiment\n",
      "Correct Answer: Supply and demand dynamics, influenced by factors like mining halvings, network activity, and macroeconomic indicators.\n",
      "\n",
      "\n",
      "\n",
      "eval_model is on: cuda:0\n",
      "input_ids are on: cuda:0\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is a Bitcoin hard fork?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "A Bitcoin hard fork is a significant change to the Bitcoin protocol that creates a new, incompatible version of the blockchain. In other words, a hard fork is a change to the underlying code of the Bitcoin network that makes it impossible for nodes on the old network to validate transactions on the new network.\n",
      "\n",
      "Hard forks can be triggered by a variety of reasons, such as:\n",
      "\n",
      "1. **Technical updates**: To improve the security, scalability, or functionality of the network.\n",
      "2. **Dispute resolution**: To\n",
      "Correct Answer: A protocol upgrade that splits the blockchain into two paths: one following new rules (incompatible with old software) and the original continuing as before.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation(\"base\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s6f4z8EYmcJ6",
    "outputId": "9ff8139a-3fd5-4330-e946-1cf34fc116a4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a218a5f8a3745bb89843fd83e63f49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238257b5cb0f4a2cb71e347c26d1a3aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25cf7f72c064c7b83d8999a6810c2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/595k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2c134d9c47444fa8c33642aa96455e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462ffc969efa4279a2391c989e4f256d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset=\"SolaireOfTheSun/BioDHBWleichtVerbessertDeutsch\"\n",
    "\n",
    "data = load_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hXM_ObnTu4JW",
    "outputId": "8f575ca2-2132-48ba-f6b6-0eb75720eba6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row of train: {'messages': '{\"messages\": [{\"role\": \"user\", \"content\": \"Welche gemeinsamen Merkmale unterscheiden Organismen von der unbelebten Welt und legen nahe, dass alles Leben auf einen gemeinsamen Ursprung zurückgeht?\"}, {\"role\": \"assistant\", \"content\": \"Die gemeinsamen Merkmale, die Organismen von der unbelebten Welt unterscheiden und darauf hindeuten, dass alles Leben auf einen gemeinsamen Ursprung zurückgeht, sind:  1. Organismen bestehen aus Zellen. 2. Organismen synthetisieren aus Molekülen ihrer Umwelt neue Biomoleküle. 3. Organismen entziehen ihrer Umwelt Energie und nutzen diese zur Verrichtung von Arbeit. 4. Organismen enthalten genetische Informationen in Form von DNA, die ihnen ermöglichen, sich zu entwickeln, ihre Lebensfunktionen aufrechtzuerhalten und sich fortzupflanzen. 5. Organismen verwenden einen universellen molekularen Code zum Aufbau von Proteinen gemäß ihrer genetischen Information.  Diese gemeinsamen Merkmale legen nahe, dass alles Leben auf der Erde von einer einzigen Lebensform abstammt und dass die Vielfalt an Organismen letztlich auf einen gemeinsamen Ursprung zurückgeht.\"}]}'}\n"
     ]
    }
   ],
   "source": [
    "print(\"First row of train:\", data['train'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [5018, 16727, 794, 62853, 5898, 794, 330, 882, 498, 330, 1834, 794, 330, 84616, 1557, 20602, 86, 14468, 11856, 9072, 2761, 1556, 267, 36110, 2761, 468, 2256, 94825, 77, 7367, 36170, 58401, 2073, 13672, 42862, 43569, 47893, 68, 16419, 473, 11289, 478, 676, 13582, 36170, 58401, 34428, 71, 388, 8703, 96138, 52973, 11, 13672, 10112, 7367, 2815, 9939, 1126, 86, 2357, 8892, 2234, 312, 351, 16414, 7673, 2186, 5324, 5898, 794, 330, 78191, 498, 330, 1834, 794, 330, 22960, 1556, 267, 36110, 2761, 468, 2256, 94825, 77, 9072, 2807, 383, 261, 11178, 20602, 86, 14468, 11856, 7367, 36170, 58401, 11, 3067, 10112, 17761, 36170, 58401, 901, 12333, 51332, 1466, 20649, 13, 36170, 58401, 387, 1964, 655, 4469, 1708, 4469, 304, 53816, 1901, 16046, 11, 2815, 75121, 91896, 7404, 452, 38056, 81056, 68, 10457, 79833, 13, 45097, 27867, 1708, 4469, 20350, 11640, 383, 468, 2256, 94825, 77, 26702, 31975, 18400, 12931, 11, 43132, 27922, 2815, 36170, 58401, 10112, 671, 11, 574, 16419, 3765, 3751, 8123, 2761, 36170, 58401, 51332, 1466, 16095, 11, 22850, 10112, 32457, 22622, 2752, 17692, 22243, 7420, 268, 1708, 4469, 7367, 38160, 20649, 13, 220, 47893, 68, 16419, 473, 11289, 478, 676, 13582, 36170, 58401, 20649, 54897, 387, 275, 57464, 11, 34428, 71, 388, 8703, 96138, 6529, 52973, 11, 13672, 10112, 7367, 2815, 9939, 1126, 86, 2357, 8892, 2234, 312, 351, 16414, 13, 61523, 47893, 68, 11, 13672, 10112, 6675, 84485, 6015, 9332, 2073, 53816, 79822, 20582, 944, 20350, 713, 97473, 48750, 11, 20649, 468, 80848, 1565, 2815, 16261, 7215, 288, 380, 17007, 6675, 36170, 58401, 58786, 46077, 2073, 19390, 59258, 11, 1536, 95818, 668, 4173, 295, 10782, 91180, 53128, 268, 12666, 81360, 20603, 17855, 919, 76, 5100, 6675, 9500, 268, 10021, 32076, 273, 59337, 13, 55168, 9939, 2779, 406, 77, 15760, 42862, 43569, 83297, 11, 29323, 3675, 6529, 81229, 17912, 11, 4543, 3453, 6383, 72079, 6675, 36170, 58401, 7367, 60885, 2761, 9267, 2807, 58496, 303, 268, 24218, 86, 3903, 6529, 2197, 1466, 5797, 1210, 92, 14316]\n",
      "Decoded Text: {\"messages\": [{\"role\": \"user\", \"content\": \"Welche Auswirkungen hat der Anstieg der Wassertemperaturen auf Korallen und wie könnten Experimente zum Hitzestress bei Korallen Vorhersagen darüber treffen, wie sie auf die Erderwärmung reagieren?\"}, {\"role\": \"assistant\", \"content\": \"Der Anstieg der Wassertemperaturen hat verheerende Auswirkungen auf Korallen, da sie zur Korallenbleiche führen können. Korallen beherbergen Algen in ihren Zellen, die ihnen wichtige Nährstoffe liefern. Wenn diese Algen durch hohe Wassertemperaturen geschädigt werden, stoßen die Korallen sie ab, was zum Absterben der Korallen führen kann, wenn sie keine widerstandsfähigeren Algen aufnehmen können.  Experimente zum Hitzestress bei Korallen können dazu beitragen, Vorhersagen darüber zu treffen, wie sie auf die Erderwärmung reagieren. Durch Experimente, wie sie von Rachael Bay und ihren Mitarbeitern durchgeführt wurden, können Wissenschaftler die Hitzeresistenz von Korallen untersuchen und feststellen, ob bestimmte genetische Eigenschaften oder unterschiedliche Expressionsmuster von Genen eine Rolle spielen. Diese Erkenntnisse könnten helfen, Strategien zu entwickeln, um den Verlust von Korallen aufgrund der sich verändernden Umwelt zu begrenzen.\"}]}\n"
     ]
    }
   ],
   "source": [
    "text = data['train'][0]['messages']\n",
    "\n",
    "tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "decoded_text = tokenizer.decode(tokens)\n",
    "\n",
    "print(\"Token IDs:\", tokens)\n",
    "print(\"Decoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dr0Aw9RRPxl3",
    "outputId": "41d215c4-68ce-4e40-82bd-100acc5adf93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID\n"
     ]
    }
   ],
   "source": [
    "model_name = model_id.split(\"/\")[-1]\n",
    "dataset_name = dataset.split(\"/\")[-1]\n",
    "\n",
    "epochs=1\n",
    "context_length = 1024\n",
    "grad_accum=1\n",
    "batch_size=1\n",
    "fine_tune_tag='DHBW-Bio-Deutsch-EducationAID'\n",
    "save_dir = f'./results/{model_name}_{dataset_name}_{epochs}_epochs_{context_length}_length-{fine_tune_tag}'\n",
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "wsEO0n1ShaIJ"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import os\n",
    "\n",
    "class LoggingCallback(transformers.TrainerCallback):\n",
    "    def __init__(self, log_file_path):\n",
    "        self.log_file_path = log_file_path\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "    def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n",
    "        with open(self.log_file_path, 'a') as f:\n",
    "            if 'loss' in logs:\n",
    "                f.write(f\"Step: {state.global_step}, Training Loss: {logs['loss']}\\n\")\n",
    "            if 'eval_loss' in logs:\n",
    "                f.write(f\"Step: {state.global_step}, Eval Loss: {logs['eval_loss']}\\n\")\n",
    "            f.flush() \n",
    "\n",
    "        \n",
    "        if state.global_step % int(args.save_steps) == 0:\n",
    "           \n",
    "            if state.best_model_checkpoint:\n",
    "                checkpoint_dir = state.best_model_checkpoint\n",
    "            else:\n",
    "               \n",
    "                checkpoint_dir = os.path.join(args.output_dir, f\"checkpoint-{state.global_step}\")\n",
    "    \n",
    "          \n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "          \n",
    "            current_trainable_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n",
    "            current_trainable_params_state_dict = {n: p.data for n, p in current_trainable_params.items()}\n",
    "            file_path = os.path.join(checkpoint_dir, \"trainable_params.bin\")\n",
    "            torch.save(current_trainable_params_state_dict, file_path)\n",
    "\n",
    "\n",
    "log_file_path = os.path.join(cache_dir, \"training_logs.txt\")\n",
    "\n",
    "\n",
    "logging_callback = LoggingCallback(log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "B3COkBi2Lx2x"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2c1aad10ad41cca78a2343205eac68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2254 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11539a9f4a304d3eba2b9302957c78c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    \n",
    "    dataset_text_field=\"messages\",\n",
    "    max_seq_length=context_length,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"test\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "       \n",
    "        save_steps=50,\n",
    "        logging_steps=1,\n",
    "        num_train_epochs=epochs,\n",
    "        output_dir=save_dir,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        do_eval=True,\n",
    "        eval_steps=0.2,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=grad_accum,\n",
    "        log_level=\"debug\",\n",
    "\n",
    "        bf16=True, # Fuer Ampere GPUs\n",
    "        max_grad_norm=0.3,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        hub_private_repo=True,\n",
    "        warmup_ratio=0.03, \n",
    "        optim=\"adamw_torch\", \n",
    "        learning_rate=1e-4, \n",
    "    ),\n",
    "    callbacks=[logging_callback], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently training with a batch size of: 1\n",
      "***** Running training *****\n",
      "  Num examples = 2,254\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2,254\n",
      "  Number of trainable parameters = 20,971,520\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240419_152924-otx90eq4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thomaslang50001/huggingface/runs/otx90eq4' target=\"_blank\">lively-wave-26</a></strong> to <a href='https://wandb.ai/thomaslang50001/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thomaslang50001/huggingface' target=\"_blank\">https://wandb.ai/thomaslang50001/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thomaslang50001/huggingface/runs/otx90eq4' target=\"_blank\">https://wandb.ai/thomaslang50001/huggingface/runs/otx90eq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2254' max='2254' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2254/2254 14:21, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>451</td>\n",
       "      <td>0.869200</td>\n",
       "      <td>1.190262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>902</td>\n",
       "      <td>0.736200</td>\n",
       "      <td>1.176631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1353</td>\n",
       "      <td>0.968400</td>\n",
       "      <td>1.143909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1804</td>\n",
       "      <td>1.136800</td>\n",
       "      <td>1.132491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-50 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-50\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-50/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-100 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-100\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-100/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-150 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-150\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-150/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-200 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-200\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-200/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-250 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-250\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-250/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-300 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-300\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-300/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-350 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-350\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-350/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-400 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-400\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-400/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-450 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-450\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-450/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 530\n",
      "  Batch size = 1\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-500/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-550 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-550\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-550/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-550/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-600 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-600\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-600/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-650 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-650\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-650/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-650/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-700 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-700\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-700/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-750 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-750\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-750/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-750/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-800 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-800\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-800/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-850 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-850\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-850/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-850/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-900 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-900\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-900/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-900/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 530\n",
      "  Batch size = 1\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-950 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-950\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-950/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-950/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1000\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1000/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1050 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1050\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1050/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1050/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1100 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1100\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1100/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1100/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1150 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1150\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1150/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1150/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1200 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1200\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1200/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1250 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1250\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1250/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1250/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1300 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1300\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1300/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1300/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1350 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1350\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1350/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1350/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 530\n",
      "  Batch size = 1\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1400 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1400\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1400/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1450 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1450\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1450/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1450/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1500/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1550 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1550\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1550/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1550/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1600 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1600\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1600/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1650 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1650\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1650/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1650/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1700 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1700\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1700/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1700/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1750 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1750\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1750/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1750/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1800 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1800\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1800/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1800/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 530\n",
      "  Batch size = 1\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1850 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1850\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1850/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1850/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1900 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1900\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1900/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1900/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1950 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1950\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1950/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-1950/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-2000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-2000\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-2000/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-2050 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-2050\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-2050/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-2050/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-2100 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-2100\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-2100/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-2100/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-2150 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-2150\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-2150/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-2150/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-2200 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-2200\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-2200/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-2200/special_tokens_map.json\n",
      "Checkpoint destination directory ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-2250 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-2250\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-2250/tokenizer_config.json\n",
      "Special tokens file saved in ./results/Meta-Llama-3-8B-Instruct_BioDHBWleichtVerbessertDeutsch_1_epochs_1024_length-DHBW-Bio-Deutsch-EducationAID/checkpoint-2250/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2254, training_loss=1.0301220236410582, metrics={'train_runtime': 901.6109, 'train_samples_per_second': 2.5, 'train_steps_per_second': 2.5, 'total_flos': 2.9160981321744384e+16, 'train_loss': 1.0301220236410582, 'epoch': 1.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.use_cache = False \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainable_params_state_dict = {n: p.data for n, p in model.named_parameters() if p.requires_grad}\n",
    "\n",
    "\n",
    "final_save_path = os.path.join(save_dir, \"trainable_params_final.bin\")\n",
    "torch.save(trainable_params_state_dict, final_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZZo5_-YXoAKt",
    "outputId": "11587512-7cfa-4436-c7b8-118a24c15c35"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.51.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (159 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.5/159.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (305 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.51.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.1 cycler-0.12.1 fonttools-4.51.0 kiwisolver-1.4.5 matplotlib-3.8.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "sVN2864jmzvA",
    "outputId": "abb2a93d-dfdc-4a36-f13a-377884ff36a3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "train_steps = []\n",
    "eval_steps = []\n",
    "\n",
    "\n",
    "for entry in trainer.state.log_history:\n",
    "    if 'loss' in entry:\n",
    "        train_losses.append(entry['loss'])\n",
    "        train_steps.append(entry['step'])\n",
    "    if 'eval_loss' in entry:\n",
    "        eval_losses.append(entry['eval_loss'])\n",
    "        eval_steps.append(entry['step'])\n",
    "\n",
    "\n",
    "plt.plot(train_steps, train_losses, label='Train Loss')\n",
    "plt.plot(eval_steps, eval_losses, label='Eval Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_model is on: cuda:0\n",
      "input_ids are on: cuda:0\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What does Bitcoin mining achieve?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitcoin "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mining is the process of verifying and adding new transactions to the public ledger, known as the blockchain, in the Bitcoin network. Miners use powerful computers to solve complex mathematical problems, which helps to secure the network and verify transactions. The process of mining achieves several key goals:\n",
      "\n",
      "1. **Verification of transactions**: Miners verify the validity of each transaction, ensuring that the sender has the necessary funds, and that the transaction is legitimate.\n",
      "2. **Creation of new blocks**: Miners group verified\n",
      "Correct Answer: Validates transactions and secures the network by adding new blocks to the blockchain through solving SHA-256 hash problems.\n",
      "\n",
      "\n",
      "\n",
      "eval_model is on: cuda:0\n",
      "input_ids are on: cuda:0\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How are Bitcoin transactions verified?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Bitcoin transactions are verified through a process called \"mining,\" which is performed by specialized computers called \"miners.\" Miners use powerful computers to solve complex mathematical problems, which helps to secure the Bitcoin network and verify transactions.\n",
      "\n",
      "Here's a step-by-step overview of how Bitcoin transactions are verified:\n",
      "\n",
      "1. **Transaction creation**: When a user wants to send Bitcoin to another user, they create a transaction and broadcast it to the network.\n",
      "2. **Verification**: Miners collect and verify the transactions,\n",
      "Correct Answer: Through digital signatures created with a sender's private key and confirmed by network nodes using the corresponding public key.\n",
      "\n",
      "\n",
      "\n",
      "eval_model is on: cuda:0\n",
      "input_ids are on: cuda:0\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the function of a Bitcoin wallet?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "A Bitcoin wallet is a software program or physical device that allows users to send, receive, and store Bitcoins. It is essentially a digital container that holds the private keys needed to access and manage a user's Bitcoin funds.\n",
      "\n",
      "The primary functions of a Bitcoin wallet include:\n",
      "\n",
      "1. **Private key management**: A Bitcoin wallet generates and stores private keys, which are used to sign transactions and prove ownership of a specific Bitcoin address.\n",
      "2. **Address management**: A Bitcoin wallet can generate and manage multiple Bitcoin addresses\n",
      "Correct Answer: Manages private keys for signing transactions and tracks associated public addresses for receiving Bitcoin.\n",
      "\n",
      "\n",
      "\n",
      "eval_model is on: cuda:0\n",
      "input_ids are on: cuda:0\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What triggers Bitcoin's price change?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Bitcoin's price is influenced by a complex array of factors, making it difficult to pinpoint a single trigger for price changes. However, some of the key factors that can impact Bitcoin's price include:\n",
      "\n",
      "1. **Supply and demand**: The balance between the number of people willing to buy and sell Bitcoin can cause its price to fluctuate.\n",
      "2. **Market sentiment**: Investor emotions, such as fear, greed, and optimism, can drive market trends and influence price movements.\n",
      "3. **Regulatory environment\n",
      "Correct Answer: Supply and demand dynamics, influenced by factors like mining halvings, network activity, and macroeconomic indicators.\n",
      "\n",
      "\n",
      "\n",
      "eval_model is on: cuda:0\n",
      "input_ids are on: cuda:0\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is a Bitcoin hard fork?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "A Bitcoin hard fork is a situation where a new version of the Bitcoin protocol is created, which is incompatible with the previous version. This means that the new version of the protocol will have different rules and features than the previous one, and nodes that run the new version will not be able to communicate with nodes that run the old version.\n",
      "\n",
      "In a hard fork, the blockchain is split into two separate chains, with each chain having its own set of rules and features. This can happen when a group of\n",
      "Correct Answer: A protocol upgrade that splits the blockchain into two paths: one following new rules (incompatible with old software) and the original continuing as before.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation(\"base\", tokenizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = save_dir + '/checkpoint-1'\n",
    "# evaluation(\"fine-tuned\", checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iJ0xWylUC1lb",
    "outputId": "58672d9d-7c17-45c5-ddc8-a13cc7512d78"
   },
   "outputs": [],
   "source": [
    "# checkpoint = save_dir + '/checkpoint-15'\n",
    "# evaluation(\"fine-tuned\", checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = save_dir + '/checkpoint-30'\n",
    "# evaluation(\"fine-tuned\", checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = save_dir + '/checkpoint-45'\n",
    "# evaluation(\"fine-tuned\", checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = save_dir + '/checkpoint-60'\n",
    "# evaluation(\"fine-tuned\", checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "VVF0R6ZhBlaf"
   },
   "outputs": [],
   "source": [
    "adapter_model = f\"SolaireOfTheSun/{model_name}-{fine_tune_tag}-adapters\"\n",
    "new_model = f\"SolaireOfTheSun/{model_name}-{fine_tune_tag}\" #adjust 'Trelis' to your HuggingFace organisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "e19FlnnCBlrM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(f\"{model_name}-{fine_tune_tag}-adapters-local\", push_to_hub=True, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297b3024b08d443291766c7bbee3fb81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:834: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--NousResearch--Meta-Llama-3-8B-Instruct/snapshots/8238378c1ceb9e54cf8d927813c7394be99d6984/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "Uploading the following files to SolaireOfTheSun/Meta-Llama-3-8B-Instruct-DHBW-Bio-Deutsch-EducationAID-adapters: README.md,adapter_model.safetensors,adapter_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c1b12bf424a45538791e5be3db1dd36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/SolaireOfTheSun/Meta-Llama-3-8B-Instruct-DHBW-Bio-Deutsch-EducationAID-adapters/commit/480c0156b6e80dc07dbf090e5b71395e21034014', commit_message='Upload model', commit_description='', oid='480c0156b6e80dc07dbf090e5b71395e21034014', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(adapter_model, use_auth_token=True, max_shard_size=\"10GB\", use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5528c53527e4ed0afe0489da7d8b4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainable_params_final.bin:   0%|          | 0.00/42.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded trainable_params_final.bin to SolaireOfTheSun/Meta-Llama-3-8B-Instruct-DHBW-Bio-Deutsch-EducationAID-adapters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from huggingface_hub import HfApi, create_repo, create_branch\n",
    "\n",
    "create_repo(new_model, private=True)\n",
    "\n",
    "create_branch(new_model, repo_type=\"model\", branch=\"gguf\")\n",
    "\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "\n",
    "repo_id = adapter_model\n",
    "\n",
    "\n",
    "local_file_paths = [\n",
    "    save_dir + \"/trainable_params_final.bin\",\n",
    "]\n",
    "\n",
    "\n",
    "for local_file_path in local_file_paths:\n",
    "\n",
    "    file_name = local_file_path.split(\"/\")[-1]\n",
    "\n",
    "    \n",
    "    path_in_repo = file_name\n",
    "\n",
    "    \n",
    "    api.upload_file(\n",
    "        path_or_fileobj=local_file_path,\n",
    "        path_in_repo=path_in_repo,\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"model\",  \n",
    "    )\n",
    "    print(f\"Uploaded {file_name} to {repo_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.temperature=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Meta-Llama-3-8B-Instruct-DHBW-Bio-Deutsch-EducationAID-local/config.json\n",
      "Configuration saved in Meta-Llama-3-8B-Instruct-DHBW-Bio-Deutsch-EducationAID-local/generation_config.json\n",
      "The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at Meta-Llama-3-8B-Instruct-DHBW-Bio-Deutsch-EducationAID-local/model.safetensors.index.json.\n",
      "tokenizer config file saved in Meta-Llama-3-8B-Instruct-DHBW-Bio-Deutsch-EducationAID-local/tokenizer_config.json\n",
      "Special tokens file saved in Meta-Llama-3-8B-Instruct-DHBW-Bio-Deutsch-EducationAID-local/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Meta-Llama-3-8B-Instruct-DHBW-Bio-Deutsch-EducationAID-local/tokenizer_config.json',\n",
       " 'Meta-Llama-3-8B-Instruct-DHBW-Bio-Deutsch-EducationAID-local/special_tokens_map.json',\n",
       " 'Meta-Llama-3-8B-Instruct-DHBW-Bio-Deutsch-EducationAID-local/tokenizer.json')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(f\"{model_name}-{fine_tune_tag}-local\")\n",
    "\n",
    "\n",
    "tokenizer.save_pretrained(f\"{model_name}-{fine_tune_tag}-local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "buqIU-9VJxVV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:834: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in /tmp/tmp3y701ksa/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/tmp3y701ksa/special_tokens_map.json\n",
      "Uploading the following files to SolaireOfTheSun/Meta-Llama-3-8B-Instruct-DHBW-Bio-Deutsch-EducationAID: tokenizer_config.json,special_tokens_map.json,tokenizer.json,README.md\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/SolaireOfTheSun/Meta-Llama-3-8B-Instruct-DHBW-Bio-Deutsch-EducationAID/commit/3d7019b64b4c492d51869e0c71be5c9bc433ac6e', commit_message='Upload tokenizer', commit_description='', oid='3d7019b64b4c492d51869e0c71be5c9bc433ac6e', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{%- set ns = namespace(found=false) -%}{%- for message in messages -%}{%- if message['role'] == 'system' -%}{%- set ns.found = true -%}{%- endif -%}{%- endfor -%}{{bos_token}}{%- if not ns.found -%}{{'You are an AI assistant and you do your best to answer all questions and requests\\n'}}{%- endif %}{%- for message in messages %}{%- if message['role'] == 'system' %}{{ message['content'] }}{%- else %}{%- if message['role'] == 'user' %}{{'Instruction:' + message['content'] + '\\n'}}{%- else %}{{'\\nOutput:' + message['content'] + '\\n<|EOT|>\\n'}}{%- endif %}{%- endif %}{%- endfor %}{% if add_generation_prompt %}{{'\\nOutput:'}}{% endif %}\"\n",
    "\n",
    "tokenizer.push_to_hub(new_model, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "vTKqNQQpIxL6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f17aa5e79b44418db9c0633cb29493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /tmp/tmpubye7ieq/config.json\n",
      "Configuration saved in /tmp/tmpubye7ieq/generation_config.json\n",
      "The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/tmpubye7ieq/model.safetensors.index.json.\n",
      "Uploading the following files to SolaireOfTheSun/Meta-Llama-3-8B-Instruct-DHBW-Bio-Deutsch-EducationAID: config.json,generation_config.json,model-00001-of-00002.safetensors,model-00002-of-00002.safetensors,model.safetensors.index.json,README.md\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7834bbd605f42a88c8854e7ad1b06e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/6.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a28871aec5341e2a93c8e7c1dd3806d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a962292e0614f00b6486f50bcfe66c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/SolaireOfTheSun/Meta-Llama-3-8B-Instruct-DHBW-Bio-Deutsch-EducationAID/commit/aff75985bf452a1b53a1fcbb2ad3063accea3024', commit_message='Upload LlamaForCausalLM', commit_description='', oid='aff75985bf452a1b53a1fcbb2ad3063accea3024', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(new_model, use_auth_token=True, max_shard_size=\"10GB\", use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download tokenizer.model. HTTP Status Code: 404\n",
      "Failed to download tokenizer.model\n",
      "Successfully downloaded README.md\n",
      "Uploaded README.md to SolaireOfTheSun/Meta-Llama-3-8B-Instruct-DHBW-Bio-Deutsch-EducationAID\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "def download_file_from_huggingface(model_id, filename, save_path):\n",
    "    url = f\"https://huggingface.co/{model_id}/resolve/main/{filename}\"\n",
    "    r = requests.get(url)\n",
    "    if r.status_code != 200:\n",
    "        print(f\"Failed to download {filename}. HTTP Status Code: {r.status_code}\")\n",
    "        return False\n",
    "    with open(os.path.join(save_path, filename), 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    return True\n",
    "\n",
    "def main():\n",
    " \n",
    "    files_to_process = [\"tokenizer.model\", \"README.md\"]\n",
    "    \n",
    "\n",
    "    save_path = \"./models\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "\n",
    "    api = HfApi()\n",
    "\n",
    " \n",
    "    repo_id = new_model  \n",
    "\n",
    "    for filename in files_to_process:\n",
    "        # Download the file\n",
    "        success = download_file_from_huggingface(model_id, filename, save_path)\n",
    "        if success:\n",
    "            print(f\"Successfully downloaded {filename}\")\n",
    "        else:\n",
    "            print(f\"Failed to download {filename}\")\n",
    "            continue  \n",
    "\n",
    "\n",
    "        local_file_path = os.path.join(save_path, filename)\n",
    "\n",
    "\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=local_file_path,\n",
    "            path_in_repo=filename, \n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"model\",  \n",
    "        )\n",
    "        print(f\"Uploaded {filename} to {repo_id}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "043f148d952e482a9029986f99161944": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "051b192ef8e540f3ac0459f74846fad3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_57c5985be925455ab3234b5f78297662",
       "IPY_MODEL_c8ebd46229a446dea6a10f48c2dd7e6d",
       "IPY_MODEL_8dc8ca5c41174894afbc7b944040a889"
      ],
      "layout": "IPY_MODEL_79ea1c4dc6824a06a92de0403f56e64b"
     }
    },
    "06c6dd0b25764569aacdfa2bb01f696f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "090f96cefcd54f17aadec2e8f5a9307b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fe2637d90e2045a29826d9023077c314",
      "placeholder": "​",
      "style": "IPY_MODEL_0a01489a78e64a14a54306a3f3c842df",
      "value": "Connecting..."
     }
    },
    "09597bc3af6144eb8ecf874aa4dce884": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0a01489a78e64a14a54306a3f3c842df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0aa8a2f53b2345a8810d5c21226ec164": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3675502942a24bf997ddc5336ed029ee",
      "placeholder": "​",
      "style": "IPY_MODEL_27e63ef320e044f48e8a7efba31c28ef",
      "value": " 414/414 [00:00&lt;00:00, 25.3kB/s]"
     }
    },
    "0aaa13230b4e443dadc3fa9ff0de3d0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0b141f85480a435983a04e95e2404e43": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f211d88f23a4fac9a20aaa2262240f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "10e22f0387474784b426d4d2f6c074cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "13817543df9d468abd3b03b13992228a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "140ca136ec2044b98f3dbc450a9f5c32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b141f85480a435983a04e95e2404e43",
      "placeholder": "​",
      "style": "IPY_MODEL_329d4c2037d94882973c778e43057c27",
      "value": "Downloading (…)/main/tokenizer.json: 100%"
     }
    },
    "14c7809be6de472dad580fb54ad9ca7b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16cdf8842b33401887430737f31a8a64": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "18180c98496e48f2861cade3dbaf8364": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aaf181f19b93404f9553cb222e531ce9",
      "max": 560,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_86587e97d3c043c99495c35d7505884b",
      "value": 560
     }
    },
    "1c66f0390e8f4621bf20e07b6fc30f85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_140ca136ec2044b98f3dbc450a9f5c32",
       "IPY_MODEL_f83fa03c74ac432792a184dc59268b11",
       "IPY_MODEL_fa46c7632a6b49e3987eaa9534a86bbf"
      ],
      "layout": "IPY_MODEL_260af2c5f38942388d3fb53a75405190"
     }
    },
    "20647bc1b50440be922754fdd7ed36b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d965f10bc3e34578b33c6cbe60913d93",
       "IPY_MODEL_2b0e8656052546adbec382ac6352b058",
       "IPY_MODEL_0aa8a2f53b2345a8810d5c21226ec164"
      ],
      "layout": "IPY_MODEL_ae74ceb23f024206b63118c37a051a79"
     }
    },
    "2267f55d15d04a43959f37d52e786984": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "260af2c5f38942388d3fb53a75405190": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26dab225c11c40d7b36d41f46d091e01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "27e63ef320e044f48e8a7efba31c28ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "27fab833dd854afda14c32ff8c50bb1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2b0e8656052546adbec382ac6352b058": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff6bbe979c324d4d958207f16cfa5980",
      "max": 414,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_45999b46ef6741e5bb0f80464bbadad4",
      "value": 414
     }
    },
    "2ef6ac7be9464d128f89b69b28e25b40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2fab06b6a0e94d028012babbc5a32d24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_16cdf8842b33401887430737f31a8a64",
      "placeholder": "​",
      "style": "IPY_MODEL_0aaa13230b4e443dadc3fa9ff0de3d0e",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "310c95b2dc504cf09322cb7ba891760a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "329d4c2037d94882973c778e43057c27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3675502942a24bf997ddc5336ed029ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36797c5c01ba4fbd9944710b6b570c5e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "387a6dba93c54678a3a5404210afd04c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e5923556c18240578703e5d443e6c81c",
       "IPY_MODEL_e909f9afd82d423a967dda899fb2bf1b",
       "IPY_MODEL_465bf049acb64c52933fbfe50f7d4ec7"
      ],
      "layout": "IPY_MODEL_ac6f2cd6604c4c9996ca09e233dd6dbe"
     }
    },
    "3c03abf5267745f99f5b68479ea653d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3fbaceee3ccf4a64b80d9451d5aa5701": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45999b46ef6741e5bb0f80464bbadad4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "465bf049acb64c52933fbfe50f7d4ec7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_14c7809be6de472dad580fb54ad9ca7b",
      "placeholder": "​",
      "style": "IPY_MODEL_043f148d952e482a9029986f99161944",
      "value": " 129/129 [00:00&lt;00:00, 1.70kB/s]"
     }
    },
    "47641c5f4c8646848374b199b139336f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d900650f66f483b8269b174148659d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "520f2ae3a5ab450db519de663e619530": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "559f7b17cf8d479b89134fbac2ff0de9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8eabbb772c347a69ddc2cdeb2b4c01f",
      "max": 4400253822,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2ef6ac7be9464d128f89b69b28e25b40",
      "value": 4400253822
     }
    },
    "563634d3c8f14a0a8a3e76f503579221": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "57c5985be925455ab3234b5f78297662": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13817543df9d468abd3b03b13992228a",
      "placeholder": "​",
      "style": "IPY_MODEL_f5161848f1594f5f8892436974b7f1e6",
      "value": "Downloading (…)okenizer_config.json: 100%"
     }
    },
    "589a72488d9c4553a18761d6966c11a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5bb444edc7ae41208fd39f83fbe98616": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ef6977176435477db177f445401f8fd9",
       "IPY_MODEL_fd2ea82e052642d8bdf66662e2585168",
       "IPY_MODEL_c112af952e5b4bdab733adfc19e2ffec",
       "IPY_MODEL_a5c6c3fbf2174628870f3755ce75e48b"
      ],
      "layout": "IPY_MODEL_902f95f0a23f4fb98156c86e9933ac59"
     }
    },
    "5c37b239dd904c398457b5db01de282b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f579084a40d49f78132406c8e8078e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_36797c5c01ba4fbd9944710b6b570c5e",
      "style": "IPY_MODEL_10e22f0387474784b426d4d2f6c074cb",
      "tooltip": ""
     }
    },
    "6433bf5eed7d4bd28a954df13b6f0e94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_589a72488d9c4553a18761d6966c11a2",
      "placeholder": "​",
      "style": "IPY_MODEL_0f211d88f23a4fac9a20aaa2262240f1",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "6834175de25d453f92164ebb336de22b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68892d09649a45d193192824dd8f37f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_848d078ed62443e297125e6532032549",
      "placeholder": "​",
      "style": "IPY_MODEL_6c056bb1731546598a7cdddbab3aaa07",
      "value": " 560/560 [00:00&lt;00:00, 35.0kB/s]"
     }
    },
    "69e55cd3bdbd45aab3e0327d10b7e77c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6c056bb1731546598a7cdddbab3aaa07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6fe96b2d78a340e5874bf90101dedbc3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "79ea1c4dc6824a06a92de0403f56e64b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b3ea7168c824ef8955cbf0c62fbe6ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7ea1b3fcf3174b41b1d38155929526fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06c6dd0b25764569aacdfa2bb01f696f",
      "placeholder": "​",
      "style": "IPY_MODEL_aeb9f39902a341c59b8d346a2558d1d1",
      "value": " 4.40G/4.40G [00:43&lt;00:00, 108MB/s]"
     }
    },
    "809122935aab423d92d71bab8a6f4c4b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "82c56ad017f44a689c53e01527228418": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "847620befc6d4b5d8ab1bd8e613d77cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "848d078ed62443e297125e6532032549": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "86587e97d3c043c99495c35d7505884b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8dc8ca5c41174894afbc7b944040a889": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d32f1a6fdec2478d9d7745c39e836c31",
      "placeholder": "​",
      "style": "IPY_MODEL_310c95b2dc504cf09322cb7ba891760a",
      "value": " 776/776 [00:00&lt;00:00, 38.5kB/s]"
     }
    },
    "8fca51abfe404f39a540b39df8df55b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "902f95f0a23f4fb98156c86e9933ac59": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "99c8287b58c74244b0edfe83c49b222a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f8d9a44cbc045a9924949b8685cec3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a3762c02dc5646748f643a912b8df6ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5c6c3fbf2174628870f3755ce75e48b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab54aea0b2cf43929af7bb4127974199",
      "placeholder": "​",
      "style": "IPY_MODEL_4d900650f66f483b8269b174148659d5",
      "value": "Login successful"
     }
    },
    "a8599580508045cfa97149579641ce83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aaf181f19b93404f9553cb222e531ce9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab54aea0b2cf43929af7bb4127974199": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac6f2cd6604c4c9996ca09e233dd6dbe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae74ceb23f024206b63118c37a051a79": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aeb9f39902a341c59b8d346a2558d1d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "af5a96810c3346c0be5b19e1f532af6f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b146dfc72bac4aa3b5aa75f7f5988083": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b5afaad1a4e74b0a923cb70fa57377dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b675c39bf53947de9387d59eb024c8bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b90890729a954284ac2e6d1a7289d488": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3fbaceee3ccf4a64b80d9451d5aa5701",
      "placeholder": "​",
      "style": "IPY_MODEL_b5afaad1a4e74b0a923cb70fa57377dc",
      "value": "Downloading pytorch_model.bin: 100%"
     }
    },
    "bcfb9c09464546b6aeca67d53badedcf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c112af952e5b4bdab733adfc19e2ffec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b675c39bf53947de9387d59eb024c8bb",
      "placeholder": "​",
      "style": "IPY_MODEL_f8c1ee39e2604e5789a74a16555514ed",
      "value": "Your token has been saved to /root/.cache/huggingface/token"
     }
    },
    "c3b70e90f9234e90bb01dac192725d85": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c7f822a2863d45698f4a172fa3d720b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b90890729a954284ac2e6d1a7289d488",
       "IPY_MODEL_559f7b17cf8d479b89134fbac2ff0de9",
       "IPY_MODEL_7ea1b3fcf3174b41b1d38155929526fb"
      ],
      "layout": "IPY_MODEL_bcfb9c09464546b6aeca67d53badedcf"
     }
    },
    "c8dd41732ce7463694555f3486afe470": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6433bf5eed7d4bd28a954df13b6f0e94",
       "IPY_MODEL_18180c98496e48f2861cade3dbaf8364",
       "IPY_MODEL_68892d09649a45d193192824dd8f37f5"
      ],
      "layout": "IPY_MODEL_6834175de25d453f92164ebb336de22b"
     }
    },
    "c8ebd46229a446dea6a10f48c2dd7e6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c37b239dd904c398457b5db01de282b",
      "max": 776,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dbb63cfdc16741efac758d83d227da49",
      "value": 776
     }
    },
    "cd321a86a74842cc804f7647d8182eb1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_82c56ad017f44a689c53e01527228418",
      "max": 499723,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_563634d3c8f14a0a8a3e76f503579221",
      "value": 499723
     }
    },
    "ce690f5848a347f0860e9c3d1704f021": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d32f1a6fdec2478d9d7745c39e836c31": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5548fc995554e51adadfca425377d54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a3762c02dc5646748f643a912b8df6ee",
      "placeholder": "​",
      "style": "IPY_MODEL_7b3ea7168c824ef8955cbf0c62fbe6ae",
      "value": "Downloading tokenizer.model: 100%"
     }
    },
    "d8eabbb772c347a69ddc2cdeb2b4c01f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d94c967a6afc4e708805a7848b171bad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d965f10bc3e34578b33c6cbe60913d93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3b70e90f9234e90bb01dac192725d85",
      "placeholder": "​",
      "style": "IPY_MODEL_9f8d9a44cbc045a9924949b8685cec3c",
      "value": "Downloading (…)cial_tokens_map.json: 100%"
     }
    },
    "da3974c730cb4ed49097fa8eba66a3d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d5548fc995554e51adadfca425377d54",
       "IPY_MODEL_cd321a86a74842cc804f7647d8182eb1",
       "IPY_MODEL_e2ee97a4ff7f4317af7ee49d4b144aed"
      ],
      "layout": "IPY_MODEL_47641c5f4c8646848374b199b139336f"
     }
    },
    "dbb63cfdc16741efac758d83d227da49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e2ee97a4ff7f4317af7ee49d4b144aed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99c8287b58c74244b0edfe83c49b222a",
      "placeholder": "​",
      "style": "IPY_MODEL_2267f55d15d04a43959f37d52e786984",
      "value": " 500k/500k [00:00&lt;00:00, 27.6MB/s]"
     }
    },
    "e5923556c18240578703e5d443e6c81c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8fca51abfe404f39a540b39df8df55b0",
      "placeholder": "​",
      "style": "IPY_MODEL_a8599580508045cfa97149579641ce83",
      "value": "Downloading (…)neration_config.json: 100%"
     }
    },
    "e61ba69b900d4a50b0b7c55eb634b8ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_520f2ae3a5ab450db519de663e619530",
      "style": "IPY_MODEL_69e55cd3bdbd45aab3e0327d10b7e77c",
      "value": true
     }
    },
    "e909f9afd82d423a967dda899fb2bf1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce690f5848a347f0860e9c3d1704f021",
      "max": 129,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_09597bc3af6144eb8ecf874aa4dce884",
      "value": 129
     }
    },
    "e959704cc7344a43ac3c5b9b9ada5ce4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef6977176435477db177f445401f8fd9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af5a96810c3346c0be5b19e1f532af6f",
      "placeholder": "​",
      "style": "IPY_MODEL_b146dfc72bac4aa3b5aa75f7f5988083",
      "value": "Token is valid (permission: write)."
     }
    },
    "f096f569b14a49dabc756943593ee15a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f414e130e49e44fca79fe125bebebadf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f5161848f1594f5f8892436974b7f1e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f83fa03c74ac432792a184dc59268b11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e959704cc7344a43ac3c5b9b9ada5ce4",
      "max": 1842767,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_26dab225c11c40d7b36d41f46d091e01",
      "value": 1842767
     }
    },
    "f8c1ee39e2604e5789a74a16555514ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f95312389a644dd981411afdebd08a55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_809122935aab423d92d71bab8a6f4c4b",
      "placeholder": "​",
      "style": "IPY_MODEL_6fe96b2d78a340e5874bf90101dedbc3",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "fa46c7632a6b49e3987eaa9534a86bbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c03abf5267745f99f5b68479ea653d0",
      "placeholder": "​",
      "style": "IPY_MODEL_f414e130e49e44fca79fe125bebebadf",
      "value": " 1.84M/1.84M [00:00&lt;00:00, 7.14MB/s]"
     }
    },
    "fd2ea82e052642d8bdf66662e2585168": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f096f569b14a49dabc756943593ee15a",
      "placeholder": "​",
      "style": "IPY_MODEL_847620befc6d4b5d8ab1bd8e613d77cf",
      "value": "Your token has been saved in your configured git credential helpers (store)."
     }
    },
    "fdb27fccde464d4e9420d87bfb465e69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_d94c967a6afc4e708805a7848b171bad",
      "placeholder": "​",
      "style": "IPY_MODEL_27fab833dd854afda14c32ff8c50bb1a",
      "value": ""
     }
    },
    "fe2637d90e2045a29826d9023077c314": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff6bbe979c324d4d958207f16cfa5980": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
